{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bag Of Words\n",
        "\n",
        "Bag of Words (BoW) is a language modeling technique used in natural language processing (NLP) to represent a text document as a bag of its words, disregarding grammar and word order, but keeping track of their frequency.\n",
        "\n",
        "The basic idea behind BoW is to represent a document as a set of its constituent words, and count how many times each word appears in the document. This results in a sparse vector representation of the document, where the vector is as long as the vocabulary size, and each dimension corresponds to a unique word in the vocabulary.\n",
        "\n",
        "The BoW technique is often used as a feature extraction method in various NLP tasks, such as text classification, sentiment analysis, and information retrieval. However, it suffers from some limitations, such as not capturing the semantic relationships between words and the context in which they appear. This has led to the development of more advanced techniques, such as word embeddings and deep learning models, that attempt to overcome these limitations.\n",
        "\n",
        "**Note**\n",
        "* This notebook is higly inspired by \n",
        "* * [Bag Of Word: Natural Language Processing](https://youtu.be/irzVuSO8o4g)\n",
        "* * [Creating Bag Of Words From S](https://www.askpython.com/python/examples/bag-of-words-model-from-scratch)"
      ],
      "metadata": {
        "id": "3p1ld7WscrrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "nltk.download('punkt') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0h6tCy4ujei",
        "outputId": "8bfc8a1a-b451-4366-c227-a5ee429b8f3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets assume we have this sentence\n",
        "* She loves pizza, pizza is delicious.\n",
        "* She is a good person.\n",
        "* good people are the best."
      ],
      "metadata": {
        "id": "yXBAy-9et5uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\"She loves pizza, pizza is delicious.\",\n",
        "        \"She is a good person.\",\n",
        "        \"good people are the best.\"]"
      ],
      "metadata": {
        "id": "_BSF5c42uKJN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its a small case, so lets just manually type the unique words in the combined sentences into a dictionary "
      ],
      "metadata": {
        "id": "TW53RfgRuG7V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kuWibZshQpo6"
      },
      "outputs": [],
      "source": [
        "identifiers = {\"she\", \"loves\", \"pizza\", \"is\" ,\"delicious\" ,\"a\", \"good\", \"person\", \"people\", \"are\" ,\"the\" ,\"best\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will try to break the words. For example our sentence is `She loves pizza, pizza is delicious.`. What we want is `[\"She\" , \"loves\" , \"pizza\" , \"piazza\" , \"is\" , \"delicious\"]`."
      ],
      "metadata": {
        "id": "Tzf-XLI_usGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For that lets assume we first have this sentecne instead `Swag Like Ohio`, and we want `[\"Swag\" , \"Like\" , \"Ohio\"]`"
      ],
      "metadata": {
        "id": "eZ-U9xAFvJgc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our first step should be to break this sentence first"
      ],
      "metadata": {
        "id": "oFy5Zd_Rvt6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.tokenize.word_tokenize(\"Swag Like Ohio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l55J7NTkvf9z",
        "outputId": "b3ef1e9f-e7ef-4232-8ec6-c44ec1b75b18"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Swag', 'Like', 'Ohio']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "and it was pretty straight forward\n",
        "\n",
        "Now what if the sentence was `[Dang Like Ohio , Swag Like Ohio]` and the expected output was `[\"Dang\" , \"Like\" , \"Ohio\" , \"Swag\" , \"Like\" , \"Ohio\"]` You would say, just tokenize them again "
      ],
      "metadata": {
        "id": "ZN1F2T0Lv6e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.tokenize.word_tokenize(\"Dang Like Ohio , Swag Like Ohio\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1BA58BDwaRp",
        "outputId": "b1438d55-7ce1-42d3-8ca0-7cfe16ec56fc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dang', 'Like', 'Ohio', ',', 'Swag', 'Like', 'Ohio']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ooops, we got an extra `,` here. But we dont want comas. what we can do is to only select letters that are `alpha` using the `str.alpha`."
      ],
      "metadata": {
        "id": "H6gwQKFPwfoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[word.lower() for word in nltk.tokenize.word_tokenize(\"Dang Like Ohio , Swag Like Ohio\") if word.isalpha() ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6Tp1jKizKMv",
        "outputId": "9cb46ee6-bf99-4d59-a67a-4c059e220233"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dang', 'like', 'ohio', 'swag', 'like', 'ohio']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And that is what something we wanted. But we need to do this for all sentences."
      ],
      "metadata": {
        "id": "mo4QW9FFzZCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "vocab = []\n",
        "for sent in data:\n",
        "    sentence = [w.lower() for w in nltk.tokenize.word_tokenize(sent) if w.isalpha() ]\n",
        "    sentences.append(sentence)\n",
        "    for word in sentence:\n",
        "        if word not in vocab:\n",
        "            vocab.append(word)"
      ],
      "metadata": {
        "id": "inLfD1OuzkYf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaS4G5OZzvcf",
        "outputId": "94ebf366-81eb-4120-e44d-9d2281815d78"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['she',\n",
              " 'loves',\n",
              " 'pizza',\n",
              " 'is',\n",
              " 'delicious',\n",
              " 'a',\n",
              " 'good',\n",
              " 'person',\n",
              " 'people',\n",
              " 'are',\n",
              " 'the',\n",
              " 'best']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KG1enzRQzyou",
        "outputId": "10894031-10f1-40f9-c8b4-62c7a252ad96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['she', 'loves', 'pizza', 'pizza', 'is', 'delicious'],\n",
              " ['she', 'is', 'a', 'good', 'person'],\n",
              " ['good', 'people', 'are', 'the', 'best']]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to assign `index` to each word in a dictionary. So that we can use that later"
      ],
      "metadata": {
        "id": "yEqlwfAr0D74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_word = {}\n",
        "i = 0\n",
        "for word in vocab:\n",
        "    index_word[word] = i \n",
        "    i += 1"
      ],
      "metadata": {
        "id": "idYCmCxK0L2w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need to define a function that adds value into a vector"
      ],
      "metadata": {
        "id": "71nFE7Ol0SwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(sent):\n",
        "    count_dict = defaultdict(int)\n",
        "    vec = np.zeros(len(vocab))\n",
        "    for item in sent:\n",
        "        count_dict[item] += 1\n",
        "    for key,item in count_dict.items():\n",
        "        vec[index_word[key]] = item\n",
        "    return vec   "
      ],
      "metadata": {
        "id": "gKAhrmDbY6v0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we have made our `Bag Of Words`."
      ],
      "metadata": {
        "id": "yMx00XuM0fM3"
      }
    }
  ]
}
